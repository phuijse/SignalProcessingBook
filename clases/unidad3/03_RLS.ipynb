{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "from IPython.display import YouTubeVideo, HTML, Audio\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import CustomJS, ColumnDataSource, Slider\n",
    "from bokeh.plotting import Figure, show, output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimadores adaptivos parte II\n",
    "\n",
    "En esta lección veremos algunos estimadores adaptivos que extienden el filtro LMS que revisamos en la lección anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmo de Mínimos Cuadrados Recursivos (*Recursive Least Squares*, RLS)\n",
    "\n",
    "\n",
    "El algoritmo LMS minimiza el error instantaneo y es simple y eficiente. Pero en algunos casos su convergencia es demasiado lenta\n",
    "\n",
    "Podemos obtener un filtro adaptivo que converge más rápido si reemplazamos el error instantaneo por el error histórico\n",
    "\n",
    "Sigamos considerando un filtro tipo FIR con $L+1$ pesos que se actualizan de en cada época $n$\n",
    "\n",
    "$$\n",
    "y_n = \\sum_{k=0}^L w_{n, k} u_{n-k}\n",
    "$$\n",
    "\n",
    "El algoritmo RLS (recursive least squares) es un método online que minimiza el error histórico, es decir la suma de errores desde la muestra inicial hasta la actual\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J^H_n(\\textbf{w}) &= \\sum_{i=L}^n   \\beta^{n-i} |e_i|^2 \\nonumber \\\\\n",
    "&= \\sum_{i=L}^n \\beta^{n-i} (d_i - \\sum_{k=0}^{L} w_{i, k} u_{i-k} )^2, \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde $n$ es el índice del instante actual y $\\beta \\in [0, 1]$ es el \"factor de olvido\", que usualmente es un valor cercano pero ligeramente menor que $1$\n",
    "\n",
    "Adicionalmente se agrega un regularizador a los pesos\n",
    "\n",
    "$$\n",
    "J^w_n = \\lambda  \\| \\textbf{w}_{n} \\|^2 = \\lambda \\sum_{k=1} w_{n, k}^2\n",
    "$$\n",
    "\n",
    "Para evitar divergencias en el proceso de entrenamiento\n",
    "\n",
    "```{note} Nota\n",
    "La función de costo total del filtro RLS es la suma de error histórico y el regularizador\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solución cerrada\n",
    "\n",
    "Si derivamos la expresión e igualamos a cero obtenemos la siguiente regla\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{w}_n &= (U_n^T \\pmb{\\beta} U_n + \\lambda I)^{-1}  U_n^T \\pmb{\\beta} \\textbf{d}_n \\nonumber \\\\\n",
    "&= \\Phi_n^{-1} \\theta_n \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde reconocemos \n",
    "\n",
    "- Matriz de correlación ponderada y regularizada: $\\Phi_n = U_n^T \\pmb{\\beta} U_n + \\lambda I$\n",
    "- Vector de correalación cruzada ponderada:  $\\theta_n = U_n^T \\pmb{\\beta} \\textbf{d}_n$\n",
    "\n",
    "que se definen en función de los siguientes términos\n",
    "\n",
    "$$\n",
    "\\textbf{d}_n = \\begin{pmatrix}  d_n \\\\ d_{n-1} \\\\ \\vdots \\\\ d_{L+1} \\end{pmatrix} \\quad\n",
    "\\textbf{u}_n = \\begin{pmatrix}  u_n \\\\ u_{n-1} \\\\ \\vdots \\\\ u_{n-(L+1)} \\end{pmatrix} \\quad\n",
    "\\pmb{\\beta} = I \\begin{pmatrix} \\beta \\\\ \\beta^{1} \\\\ \\beta^{2}  \\vdots \\\\ \\beta^{n-L-1} \\end{pmatrix}\n",
    "\\quad \n",
    "U_n = \\begin{pmatrix}\n",
    "\\textbf{u}_n^T \\\\ \\textbf{u}_{n-1}^T \\\\ \\vdots \\\\ \\textbf{u}_{L+1}^T \\\\\n",
    "\\end{pmatrix} \\in \\mathbb{R}^{n - (L+1) \\times L+1}\n",
    "$$\n",
    "\n",
    "e $I$ es la matriz identidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Esta solución es similar a la del filtro de Wiener. No es fácil actualizarla a medida que llegan nuevas observaciones y además es muy costoso debido al cálculo del inverso de la matriz de correlación\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Solución recursiva\n",
    "\n",
    "En lugar de la solución cerrada, es más conveniente actualizar los pesos de forma recursiva\n",
    "\n",
    "Las condiciones iniciales son \n",
    "\n",
    "- $\\Phi_0 = \\lambda^{-1} I$\n",
    "- $\\theta_0 = 0$\n",
    "\n",
    "y luego la actualización viene dada por \n",
    "\n",
    "- $\\Phi_{n} = \\beta \\Phi_{n-1} + \\textbf{u}_n \\textbf{u}_n^T$ \n",
    "- $\\theta_{n} = \\beta \\theta_{n-1} + \\textbf{u}_n d_n $ \n",
    "- $\\textbf{w}_n = \\Phi_n^{-1} \\theta_n$\n",
    "\n",
    "Podemos evitar invertir la matriz de correlación si usamos el lema de inversión de matrices \n",
    "\n",
    "$$\n",
    "(A + UCV)^{-1} = A^{-1} - A^{-1} U (C^{-1} + VA^{-1} U)^{-1} V A^{-1}\n",
    "$$\n",
    "\n",
    "con $A = \\Phi_{n-1}^{-1}$, $C=1$, $U= \\textbf{u}_n$ y $V = \\textbf{u}_n^T$. \n",
    "\n",
    "De esta forma podemos actualizar $\\Phi_{n}^{-1}$ directamente sin tener que invertir\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Phi_{n}^{-1} &= \\left(\\beta \\Phi_{n-1} + \\textbf{u}_n \\textbf{u}_n^T \\right)^{-1} \\nonumber \\\\\n",
    "&= \\beta^{-1} \\Phi_{n-1}^{-1} - \\beta^{-2} \\frac{\\Phi_{n-1}^{-1} \\textbf{u}_n \\textbf{u}_n^T \\Phi_{n-1}^{-1} }{1 + \\beta^{-1} \\textbf{u}_n^T \\Phi_{n-1}^{-1} \\textbf{u}_n} \\nonumber \\\\\n",
    "&= \\beta^{-1} \\Phi_{n-1}^{-1} - \\beta^{-1} \\textbf{k}_n \\textbf{u}_n^T \\Phi_{n-1}^{-1}, \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde llamamos **ganancia** a \n",
    "\n",
    "$$\n",
    "\\textbf{k}_n =  \\frac{\\beta^{-1} \\Phi_{n-1}^{-1} \\textbf{u}_n }{1 + \\beta^{-1} \\textbf{u}_n^T \\Phi_{n-1}^{-1} \\textbf{u}_n}\n",
    "$$\n",
    "\n",
    "El último paso es obtener al regla de actualización de pesos\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{w}_n &= \\Phi_n^{-1} \\theta_n \\nonumber \\\\\n",
    "&=  \\Phi_n^{-1} \\left ( \\beta \\theta_{n-1} + \\textbf{u}_n d_n \\right) \\nonumber \\\\\n",
    "&=  \\left ( \\beta^{-1} \\Phi_{n-1}^{-1} - \\beta^{-1} \\textbf{k}_n \\textbf{u}_n^T \\Phi_{n-1}^{-1} \\right ) \\beta \\theta_{n-1} + \\Phi_n^{-1} \\textbf{u}_n d_n \\nonumber \\\\\n",
    "&=  \\textbf{w}_{n-1} - \\textbf{k}_n \\textbf{u}_n^T  \\textbf{w}_{n-1} + \\Phi_n^{-1} \\textbf{u}_n d_n \\nonumber \\\\\n",
    "&=  \\textbf{w}_{n-1} + \\textbf{k}_n ( d_n - \\textbf{u}_n^T  \\textbf{w}_{n-1} ) \\nonumber \\\\\n",
    "&=  \\textbf{w}_{n-1} + \\textbf{k}_n e_n \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde usamos que $\\textbf{w}_{n-1} = \\Phi_{n-1}^{-1} \\theta_{n-1}$ y $\\textbf{k}_n = \\Phi_n^{-1} \\textbf{u}_n$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```{note} \n",
    "Con esto tenemos un algoritmo de orden cuadrático en lugar de orden cúbico. Esto sigue siendo mayor que LMS que era de orden lineal pero tiene la ventaja de converger más rapidamente  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen algoritmo RLS\n",
    "\n",
    "Inicializar $\\Phi_0^{-1} = \\lambda I$ y $\\textbf{w}_0 = 0$\n",
    "\n",
    "Para $n \\in [1, \\infty]$\n",
    "\n",
    "- Calcular la ganancia\n",
    "\n",
    "$$\n",
    "\\textbf{k}_n =  \\frac{\\Phi_{n-1}^{-1} \\textbf{u}_n }{\\beta + \\textbf{u}_n^T \\Phi_{n-1}^{-1} \\textbf{u}_n}\n",
    "$$\n",
    "\n",
    "- Calcular el error\n",
    "\n",
    "$$\n",
    "e_n = d_n - \\textbf{u}_n^T  \\textbf{w}_{n-1} \n",
    "$$\n",
    "\n",
    "- Actualizar el error de pesos\n",
    "\n",
    "$$\n",
    "\\textbf{w}_n = \\textbf{w}_{n-1} + \\textbf{k}_n e_n \n",
    "$$\n",
    "\n",
    "- Actualizar el inverso de la matriz de correlación\n",
    "\n",
    "$$\n",
    "\\Phi_{n}^{-1} = \\beta^{-1} \\Phi_{n-1}^{-1} - \\beta^{-1} \\textbf{k}_n \\textbf{u}_n^T \\Phi_{n-1}^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hiperparámetros $\\beta$ y $\\lambda$\n",
    "\n",
    "El hiperparámetro $\\beta$ define la memoría efectiva del sistema y repercute en la convergencia y estabilidad del filtro. Como punto de partida se sugiere un valor de $\\beta \\approx 0.99$. En general $\\beta \\in [0.9, 1.0)$\n",
    "\n",
    "Para valores pequeños de $L$ existe el riesgo de que la matriz de correlación resulte singular afectando la estabilidad del filtro. Por esta razón inicializamos con una matriz idéntidad multiplicada por el factor $\\lambda$. Mientras más pequeño sea su valor mayor será la regularización. Se recomienda $\\lambda < 0.01/\\sigma_u^2$ donde $\\sigma_u$ es la desviación estándar de la señal de entrada\n",
    "\n",
    "En la práctica se pueden calibrar con validación cruzada al igual que $L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Filtro_RLS:\n",
    "    \n",
    "    def __init__(self, L, beta=0.99, delta=1e-2):\n",
    "        self.L = L\n",
    "        self.w = np.zeros(shape=(L+1, ))\n",
    "        self.beta = beta\n",
    "        self.delta = delta\n",
    "        self.Phi_inv = delta*np.eye(L+1)\n",
    "        \n",
    "    def update(self, un, dn):\n",
    "        # Cálculo de la ganancia\n",
    "        pi = np.dot(un.T, self.Phi_inv)\n",
    "        kn = pi.T/(self.beta + np.inner(pi, un))\n",
    "        # Actualizar el vector de pesos\n",
    "        error = dn - np.dot(self.w, un)\n",
    "        self.w += kn*error\n",
    "        # Actualizar el inverso de Phi\n",
    "        self.Phi_inv = (self.Phi_inv - np.outer(kn, pi))*self.beta**-1\n",
    "        return np.dot(self.w, un)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Aplicación: ALE con filtro RLS\n",
    "\n",
    "Veamos como reacciona el filtro RLS ante cambios bruscos usando el ejemplo de la lección pasada. Comparemos con el filtro NLMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "Fs, f0 =  100, 5\n",
    "t = np.arange(0, 3, 1/Fs)\n",
    "s = np.sin(2.0*np.pi*t*f0)\n",
    "s[t>1] += 5\n",
    "u = s + 0.5*np.random.randn(len(t))\n",
    "\n",
    "class Filtro_NLMS:\n",
    "    \n",
    "    def __init__(self, L, mu, delta=1e-6):\n",
    "        self.L = L\n",
    "        self.w = np.zeros(shape=(L+1, ))\n",
    "        self.mu = mu\n",
    "        self.delta = delta\n",
    "        \n",
    "    def update(self, un, dn):\n",
    "        unorm = np.dot(un, un) + self.delta\n",
    "        error = dn - np.dot(self.w, un)\n",
    "        self.w += 2*self.mu*error*(un/unorm)\n",
    "        return np.dot(self.w, un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 20\n",
    "lms = Filtro_NLMS(L, 0.02)\n",
    "rls = Filtro_RLS(L, 0.99, 1e-2)\n",
    "\n",
    "u_pred = np.zeros(shape=(len(u), 2))\n",
    "for k in range(L+1, len(u)):\n",
    "    u_pred[k, 0] = lms.update(u[k-L-1:k][::-1], u[k])\n",
    "    u_pred[k, 1] = rls.update(u[k-L-1:k][::-1], u[k])\n",
    "    \n",
    "p1 = Figure(plot_width=650, plot_height=250, toolbar_location=\"below\")\n",
    "p2 = Figure(plot_width=650, plot_height=250, toolbar_location=\"below\")\n",
    "\n",
    "p1.line(t, s, color='green', line_width=4, legend_label=f\"Limpia\");  \n",
    "p1.scatter(t, u, color='black', legend_label=f\"Ruidosa\")\n",
    "p1.line(t, u_pred[:, 0], color='blue', \n",
    "        line_width=2, legend_label=f\"LMS\");\n",
    "p1.line(t, u_pred[:, 1], color='red', \n",
    "        line_width=2, legend_label=f\"RLS\");\n",
    "p1.title.text = \"Señal\"\n",
    "p1.legend.location = \"bottom_right\"\n",
    "p2.line(t, (u - u_pred[:, 0])**2, color='blue', \n",
    "        line_width=2, legend_label=f\"LMS\");   \n",
    "p2.line(t, (u - u_pred[:, 1])**2, color='red', \n",
    "        line_width=2, legend_label=f\"RLS\");   \n",
    "p2.title.text = \"Error cuadrático instantaneo\"\n",
    "show(column([p1, p2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note} \n",
    "RLS es capaz de seguir los cambios de la señal en menos tiempo que el filtro LMS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmo Perceptrón (Rosemblatt 1962)\n",
    "\n",
    "El perceptrón es un filtro adaptivo para hacer **clasificación supervisada de patrones**\n",
    "\n",
    "Asumiremos que\n",
    "\n",
    "- La respuesta deseada tiene dos categorías: $d_n \\in \\{-1, +1\\}$. El perceptrón resuelve un problema de **clasificación binario**\n",
    "- La entrada es continua y de $L$ dimensiones: $u_n \\in \\mathbb{R}^L$\n",
    "- Se tienen $N$ tuplas $(u_n, d_n)$ para entrenar el filtro\n",
    "\n",
    "El filtro tiene arquitectura FIR con $L+1$ coeficientes pero se agrega una función no lineal $\\phi(\\cdot)$ en la salida del filtro\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_n &=  \\phi \\left(b + \\sum_{k=1}^{M} w_k u_{nk} \\right) \\nonumber \\\\\n",
    "&= \\phi \\left(b + \\langle \\textbf{w}, \\textbf{u}_n \\rangle \\right), \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Los coeficientes del filtro son el escalar $b$ y el vector $\\textbf{w}$\n",
    "\n",
    "Este filtro corresponde al modelo matemático de una neurona de [McCulloch y Pitts](https://link.springer.com/article/10.1007/BF02478259), el antecesor de las actuales redes neuronales profundas\n",
    "\n",
    "En la implemetación original se utilizó la siguiente función no lineal o función de activación\n",
    "\n",
    "$$\n",
    "\\phi(z) = \\text{sign}(z) = \\begin{cases} +1 & z > 0 \\\\0 & z=0\\\\-1 & z<0 \\end{cases}\n",
    "$$\n",
    "\n",
    "La siguiente figura esquematiza el modelo y su inspiración biológica\n",
    "\n",
    "<img src=\"../images/neurona.png\" width=\"700\">\n",
    "\n",
    "- Las coeficientes del filtro simulan la importancia o peso de las dendritas\n",
    "- La función no lineal simula el axón que dispara un estímulo eléctrico cuando el voltaje supera un umbral\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de la neurona artificial\n",
    "\n",
    "La neurona ajusta sus parámetros con cada ejemplo que recibe. \n",
    "\n",
    "Sea un ejemplo en particular $(d_i, u_i)$, primero se verifica la siguiente condición\n",
    "\n",
    "$$\n",
    "\\text{sign} \\left(b + \\langle \\textbf{w}, \\textbf{u}_n \\rangle \\right) \\neq d_i\n",
    "$$\n",
    "\n",
    "que corresponde a comprobar si el ejemplo está mal clasificado o no.\n",
    "\n",
    "En el caso de que se cumpla se actualizan los parámetros como\n",
    "\n",
    "$$\n",
    "\\textbf{w} =  \\textbf{w} + \\mu d_n \\textbf{u}_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b + \\mu d_n\n",
    "$$\n",
    "\n",
    "de lo contrario los parámetros se mantienen sin modificar\n",
    "\n",
    "El hiperparámetro $\\mu$ es la tasa de aprendizaje de la neurona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo perceptrón\n",
    "\n",
    "El algoritmo para entrenar la neurona artificial a un conjunto de $N$ ejemplos se conoce como **algoritmo percetrón**\n",
    "\n",
    "- Los parámetros de la neurona se inicializan en cero\n",
    "- En cada iteración del algoritmo se presenta un ejemplo \n",
    "- Se completa una época de entrenamiento cuando se han presentado los $N$ ejemplos del conjunto\n",
    "- Detenemos el entrenamiento cuando todos los ejemplos están bien clasificados o al cumplir un cierto número de épocas sin cambio\n",
    "- Se sugiere presentar los ejemplos en distinto orden en cada época para evitar sesgos y acelerar la convergencia\n",
    "\n",
    "El algoritmo perceptrón está garantizado a converger en tiempo finito si el problema es **linealmente separable**. Si el problema no es **linealmente separable** la convergencia se puede forzar disminuyendo gradualmente $\\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Interpretación como una aplicación de gradiente descendente estocástico (SGD)\n",
    "\n",
    "El algoritmo de ajuste de la neurona puede considerarse como una minimización de la siguiente función de costo\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(b, \\textbf{w} ) = \\text{max} \\Big(0 ~, - d_n ( b + \\langle \\textbf{w}, \\textbf{u}_n \\rangle) \\Big)\n",
    "$$\n",
    "\n",
    "cuya derivada es \n",
    "\n",
    "$$\n",
    "\\frac{d \\mathcal{L}}{d \\textbf{w}}  = \\begin{cases} 0 & d_n ( b + \\langle \\textbf{w}, \\textbf{u}_n \\rangle)  \\geq 0 \\\\ - d_n \\textbf{u}_n  & d_n ( b + \\langle \\textbf{w}, \\textbf{u}_n \\rangle)  < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d \\mathcal{L}}{d b}  = \\begin{cases} 0 & d_n ( b + \\langle \\textbf{w}, \\textbf{u}_n \\rangle)  \\geq 0 \\\\ - d_n   & d_n ( b + \\langle \\textbf{w}, \\textbf{u}_n \\rangle)  < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "es decir que la derivada es cero si el ejemplo está bien clasificado\n",
    "\n",
    "Notemos que si aplicamos SGD sobre esta función de costo\n",
    "\n",
    "$$\n",
    "\\textbf{w} = \\textbf{w} - \\mu \\frac{d \\mathcal{L}}{d \\textbf{w}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - \\mu \\frac{d \\mathcal{L}}{db}\n",
    "$$\n",
    "\n",
    "se recuperan las reglas de ajuste vistas anteriormente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Clasificación binaria con perceptrón\n",
    "\n",
    "La siguiente animación muestra como se ajusta el perceptrón a medida que se presentan los ejemplos para un problema linealmente separable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "# Creamos un dataset sintético\n",
    "\n",
    "N = 10 # Ejemplos por clase\n",
    "L = 2 # Dimensión de los datos\n",
    "np.random.seed(1234)\n",
    "u = np.concatenate((np.random.randn(N, L), \n",
    "                    4 + np.random.randn(N, L)))\n",
    "d = np.ones(shape=(2*N,)); \n",
    "d[:N] = -1.\n",
    "\n",
    "# Parámetros e hiperparámetros\n",
    "b = 0\n",
    "w = np.zeros(shape=(L, ))\n",
    "mu = 1e-5\n",
    "\n",
    "# Preparar animación\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(u[:N, 0], u[:N, 1], marker='x'); \n",
    "ax.scatter(u[N:, 0], u[N:, 1], marker='o')\n",
    "x_plot = np.linspace(np.amin(u), np.amax(u))\n",
    "hiperplano = lambda x, w, b, tol=1e-10 : -b/(w[1]+tol) - x*w[0]/(w[1]+tol)\n",
    "line = ax.plot(x_plot, hiperplano(x_plot, w, b), 'k-', lw=4, alpha=0.75) \n",
    "dot = ax.plot([], [], 'ko', markersize=10)\n",
    "\n",
    "# Revolvemos el dataset para evitar sesgos\n",
    "sorted_idx = np.random.permutation(2*N)\n",
    "u = u[sorted_idx]\n",
    "d = d[sorted_idx]\n",
    "\n",
    "def update_plot(n):\n",
    "    global w\n",
    "    global b\n",
    "    line[0].set_ydata(hiperplano(x_plot, w, b))\n",
    "    dn, un = d[n - len(d)*(n//len(d))], u[n - len(d)*(n//len(d))]\n",
    "    if dn*(b+np.inner(w, un)) <= 0.:\n",
    "        w += mu*dn*un\n",
    "        b += mu*dn           \n",
    "    dot[0].set_data(un[0], un[1]) # ejemplo actual\n",
    "    ax.set_title(f\"Iteración {n}, Epoca {n//len(d)}\")\n",
    "        \n",
    "\n",
    "anim = animation.FuncAnimation(fig, update_plot, frames=80, interval=500, \n",
    "                               repeat=False, blit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note} \n",
    "El hiperplano se traslada y rota (transformación lineal) cada vez que encuentra un ejemplo mal clasificado\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Más allá del perceptron \n",
    "\n",
    "- El modelo de neurona con salida sigmoide se conoce como **regresión logística**\n",
    "- Tanto el perceptrón como el regresor logístico se pueden extender a más de dos clases: **regresor softmax**\n",
    "- Conectando varias neuronas en cadena se forma lo que se conoce como una perceptrón multicapa. Este es un ejemplo de **red neuronal artificial**\n",
    "- Las redes neuronales artificiales se estudian en mayor detalle en el curso de **inteligencia artificial** (INFO257)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INFO183",
   "language": "python",
   "name": "info183"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
