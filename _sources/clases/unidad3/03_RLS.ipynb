{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import scipy.fft as sfft\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "from IPython.display import YouTubeVideo, HTML, Audio\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import CustomJS, ColumnDataSource, Slider\n",
    "from bokeh.plotting import Figure, show, output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimadores adaptivos parte II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmo Recursive Least Squares (RLS)\n",
    "\n",
    "\n",
    "\n",
    "El algoritmo LMS\n",
    "- minimiza el error instantaneo\n",
    "- es simple y eficiente \n",
    "- en algunos casos su convergencia es demasiado lenta\n",
    "\n",
    "Podemos obtener un filtro adaptivo que converge más rápido si reemplazamos el error instantaneo por el error histórico\n",
    "\n",
    "Sigamos considerando un filtro tipo FIR con $L+1$ pesos que se actualizan de en cada época\n",
    "\n",
    "$$\n",
    "y_i = \\sum_{k=0}^L w_{i, k} u_{n-k}\n",
    "$$\n",
    "\n",
    "El algoritmo RLS (recursive least squares) es un método online que minimiza el error histórico, es decir la suma de errores entre la muestra actual y la inicial\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J^H_n(\\textbf{w}) &= \\sum_{i=L}^n   \\beta^{n-i} |e_i|^2 \\nonumber \\\\\n",
    "&= \\sum_{i=L+1}^n \\beta^{n-i} (d_i - \\sum_{k=0}^{L} w_{i, k} u_{i-k} )^2, \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde $n$ es el índice del instante actual y $\\beta \\in [0, 1]$ es el \"factor de olvido\" y que usualmente es un valor cercano a $1$ \n",
    "\n",
    "Adicionalmente se agrega un regularizador a los pesos\n",
    "$$\n",
    "J^w_n = \\delta  \\| \\textbf{w}_{n} \\|^2\n",
    "$$\n",
    "\n",
    "La solución cerrada sería\n",
    "\n",
    "$$\n",
    "\\textbf{w}_n = (U_n^T \\pmb{\\beta} U_n + \\delta I)^{-1}  U_n^T \\pmb{\\beta} \\textbf{d}_n\n",
    "$$\n",
    "donde \n",
    "$$\n",
    "\\textbf{d}_n = \\begin{pmatrix}  d_n \\\\ d_{n-1} \\\\ \\vdots \\\\ d_{L+1} \\end{pmatrix} \\quad\n",
    "\\textbf{u}_n = \\begin{pmatrix}  u_n \\\\ u_{n-1} \\\\ \\vdots \\\\ u_{n-(L+1)} \\end{pmatrix} \\quad\n",
    "\\pmb{\\beta} = I \\begin{pmatrix} \\beta \\\\ \\beta^{1} \\\\ \\beta^{2}  \\vdots \\\\ \\beta^{n-L-1} \\end{pmatrix}\n",
    "\\quad \n",
    "U_n = \\begin{pmatrix}\n",
    "\\textbf{u}_n^T \\\\ \\textbf{u}_{n-1}^T \\\\ \\vdots \\\\ \\textbf{u}_{L+1}^T \\\\\n",
    "\\end{pmatrix} \\in \\mathbb{R}^{n - (L+1) \\times L+1}\n",
    "$$\n",
    "\n",
    "e $I$ es la matriz identidad. \n",
    "\n",
    "- Notemos la similitud con el filtro de Wiener\n",
    "    - Matriz de correlación ponderada y regularizada: $\\Phi_n = U_n^T \\pmb{\\beta} U_n + \\delta I$\n",
    "    - Vector de correalación cruzada ponderada:  $\\theta_n = U_n^T \\pmb{\\beta} \\textbf{d}_n$\n",
    "- Queremos recomputar esta solución cuando llegan nuevas observaciones\n",
    "- En particular queremos evitar invertir $\\Phi_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El algoritmo **RLS** propone una solución que actualiza los pesos de forma recursiva\n",
    "\n",
    "Las condiciones iniciales son \n",
    "- $\\Phi_0 = \\delta I$\n",
    "- $\\theta_0 = 0$\n",
    "\n",
    "y luego la actualización viene dada por \n",
    "\n",
    "- $\\Phi_{n} = \\beta \\Phi_{n-1} + \\textbf{u}_n \\textbf{u}_n^T$ \n",
    "- $\\theta_{n} = \\beta \\theta_{n-1} + \\textbf{u}_n d_n $ \n",
    "- $\\textbf{w}_n = \\Phi_n^{-1} \\theta_n$\n",
    "\n",
    "Que es más eficiente si actualizamos $\\Phi_{n}^{-1}$ en lugar de $\\Phi_{n}$ \n",
    "\n",
    "Usando el lema de inversión de matrices \n",
    "$$\n",
    "(A + UCV)^{-1} = A^{-1} - A^{-1} U (C^{-1} + VA^{-1} U)^{-1} V A^{-1}\n",
    "$$\n",
    "\n",
    "con $A = \\Phi_{n-1}^{-1}$, $C=1$, $U= \\textbf{u}_n$ y $V = \\textbf{u}_n^T$ entonces\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Phi_{n}^{-1} &= \\left(\\beta \\Phi_{n-1} + \\textbf{u}_n \\textbf{u}_n^T \\right)^{-1} \\nonumber \\\\\n",
    "&= \\beta^{-1} \\Phi_{n-1}^{-1} - \\beta^{-2} \\frac{\\Phi_{n-1}^{-1} \\textbf{u}_n \\textbf{u}_n^T \\Phi_{n-1}^{-1} }{1 + \\beta^{-1} \\textbf{u}_n^T \\Phi_{n-1}^{-1} \\textbf{u}_n} \\nonumber \\\\\n",
    "&= \\beta^{-1} \\Phi_{n-1}^{-1} - \\beta^{-1} \\textbf{k}_n \\textbf{u}_n^T \\Phi_{n-1}^{-1}, \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde llamamos **ganancia** a $\\textbf{k}_n$\n",
    "\n",
    "Podemos continuar para encontrar una regla de actualización recursiva para los pesos\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{w}_n &= \\Phi_n^{-1} \\theta_n \\nonumber \\\\\n",
    "&=  \\Phi_n^{-1} \\beta \\theta_{n-1} + \\Phi_n^{-1} \\textbf{u}_n d_n\\nonumber \\\\\n",
    "&=  \\Phi_{n-1}^{-1} \\theta_{n-1} - \\textbf{k}_n \\textbf{u}_n^T \\Phi_{n-1}^{-1} \\theta_{n-1} + \\Phi_n^{-1} \\textbf{u}_n d_n \\nonumber \\\\\n",
    "&=  \\textbf{w}_{n-1} - \\textbf{k}_n \\textbf{u}_n^T  \\textbf{w}_{n-1} + \\Phi_n^{-1} \\textbf{u}_n d_n \\nonumber \\\\\n",
    "&=  \\textbf{w}_{n-1} + \\textbf{k}_n ( d_n - \\textbf{u}_n^T  \\textbf{w}_{n-1} ) \\nonumber \\\\\n",
    "&=  \\textbf{w}_{n-1} + \\textbf{k}_n e_n \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "donde reemplazamos $\\textbf{w}_{n-1} = \\Phi_{n-1}^{-1} \\theta_{n-1}$ y usamos que \n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{k}_n &= \\left(\\beta^{-1} \\Phi_{n-1}^{-1} - \\beta^{-1} \\textbf{k}_n \\textbf{u}_n^T \\Phi_{n-1}^{-1} \\right)  \\textbf{u}_n \\nonumber \\\\ &= \\Phi_n^{-1} u_n \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "### Notas\n",
    "- Con esto tenemos un algoritmo de complejidad $L^2$ en vez de $L^3$\n",
    "- Esto sigue siendo mayor que LMS (complejidad $L$) pero con la ventaja de converger más rapidamente \n",
    "- En la literatura suele usarse el nombre $P_n$ para $\\Phi_n^{-1}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Resumen algoritmo RLS\n",
    "\n",
    "- Inicializar $\\Phi_0 = \\delta I$ y $\\textbf{w}_0 = 0$\n",
    "- Para $n \\in [1, \\infty]$\n",
    "    1. Calcular la ganancia\n",
    "    $$\n",
    "    \\textbf{k}_n =  \\frac{\\beta^{-1} \\Phi_{n-1}^{-1} \\textbf{u}_n }{1 + \\beta^{-1} \\textbf{u}_n^T \\Phi_{n-1}^{-1} \\textbf{u}_n}\n",
    "    $$\n",
    "    1. Calcular el error\n",
    "    $$\n",
    "    e_n = d_n - \\textbf{u}_n^T  \\textbf{w}_{n-1} \n",
    "    $$\n",
    "    1. Actualizar el error de pesos\n",
    "    $$\n",
    "    \\textbf{w}_n = \\textbf{w}_{n-1} + \\textbf{k}_n e_n \n",
    "    $$\n",
    "    1. Actualizar el inverso de la matriz de correlación\n",
    "    $$\n",
    "    \\Phi_{n}^{-1} = \\beta^{-1} \\Phi_{n-1}^{-1} - \\beta^{-1} \\textbf{k}_n \\textbf{u}_n^T \\Phi_{n-1}^{-1}\n",
    "    $$\n",
    "    \n",
    "### Recomendaciones\n",
    "- A menor $\\delta$ mayor regularización. Usar $\\delta$ pequeño para SNR bajo y $\\delta$ grande para SNR alto\n",
    "- Considerar un valor de $\\beta \\approx 0.9$ inicialmente\n",
    "- Calibre $\\delta$ y $\\beta$ usando validación cruzada\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RLS versus LMS: Tracking\n",
    "\n",
    "- Notemos la diferencia en tiempo de convergencia entre los algoritmos LMS y RLS\n",
    "- RLS es capaz de adaptarse a cambios bruscos más rápido que LMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, figsize=(9, 6))\n",
    "t, dt = np.linspace(0, 5, num=500, retstep=True)\n",
    "np.random.seed(0)\n",
    "u = np.sin(2.0*np.pi*t*5)  \n",
    "# u[250:] += 5\n",
    "u += np.array([5*np.exp(-np.absolute(tt-1)/0.2) if tt>1 else 0 for tt in t])\n",
    "u += np.array([5*np.exp(-np.absolute(tt-3)/0.2) if tt>3 else 0 for tt in t])\n",
    "u_noisy = u + 0.5*np.random.randn(len(t))\n",
    "\n",
    "def update(mu, beta, L):\n",
    "    ax[0].cla(); ax[1].cla(); \n",
    "    u_pred = np.zeros(shape=(len(u_noisy), 2))\n",
    "    #LMS\n",
    "    w = np.zeros(shape=(L+1, ))\n",
    "    for k in range(L+1, len(u_noisy)):\n",
    "        u_window = u_noisy[k-L-1:k]\n",
    "        norm = np.sum(u_window**2) + 1e-6\n",
    "        u_pred[k, 0] = np.dot(w, u_window)\n",
    "        if k < len(u):\n",
    "            w += 2*mu*(u_noisy[k] - u_pred[k, 0])*u_window/norm\n",
    "    #RLS\n",
    "    w = np.zeros(shape=(L+1, ))\n",
    "    delta = 1.\n",
    "    Phi_inv = delta*np.eye(L+1); \n",
    "    for k in range(L+1, len(u_noisy)):\n",
    "        u_window = u_noisy[k-L-1:k]\n",
    "        # Calcular ganancia\n",
    "        gain = np.dot(Phi_inv, u_window)/(beta + np.dot(np.dot(u_window.T, Phi_inv), u_window))        \n",
    "        # Calcular error\n",
    "        u_pred[k, 1] = np.dot(w, u_window)\n",
    "        err = u_noisy[k] - u_pred[k, 1]\n",
    "        # Actualizar pesos\n",
    "        w += np.dot(gain, err)\n",
    "        # Actualizar el inverso de la matriz de correlación\n",
    "        Phi_inv = (1./beta)*(1. - np.sum(gain*u_window))*Phi_inv\n",
    "    \n",
    "    ax[0].plot(t, u_noisy, 'k.', alpha=0.5); \n",
    "    ax[0].plot(t, u, 'g-', alpha=0.5, label='Real');  \n",
    "    ax[0].plot(t, u_pred[:, 0], label='LMS'); \n",
    "    ax[0].plot(t, u_pred[:, 1], label='RLS'); ax[0].legend(loc=1)\n",
    "    ax[1].plot((u - u_pred[:, 0])**2, label='LMS')\n",
    "    ax[1].plot((u - u_pred[:, 1])**2, label='RLS'); ax[1].legend()\n",
    "\n",
    "    \n",
    "interact(update, \n",
    "         mu=FloatSlider_nice(step=0.01, max=0.1, min=0.01),\n",
    "         beta=FloatSlider_nice(step=0.01, max=1., min=0.5, value=0.9),\n",
    "         L=SelectionSlider_nice(options=[1, 5, 10, 20, 50], value=10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmo Perceptrón (Rosemblatt 1962)\n",
    "\n",
    "\n",
    "- Podemos entrenar un filtro adaptivo para hacer **clasificación binaria de patrones**\n",
    "- En este caso la respuesta deseada tiene dos categorías: $d_n \\in \\{-1, +1\\}$ \n",
    "- La entrada se considera continua y de $M$ dimensiones: $u_n \\in \\mathbb{R}^M$\n",
    "- Asumimos que se tienen $N$ tuplas $(u_n, d_n)$\n",
    "- El filtro tiene arquitectura FIR con $M+1$ coeficientes pero se agrega una no linealidad $\\phi(\\cdot)$ en la salida\n",
    "$$\n",
    "\\begin{align}\n",
    "y_n &=  \\phi \\left(w_0 + \\sum_{k=1}^{M} w_k u_{nk} \\right) \\nonumber \\\\\n",
    "&= \\phi \\left(\\langle \\textbf{w}, \\text{concat}(1, u_n) \\rangle \\right), \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "donde podemos usar $\\phi(z) = \\text{sign}(z) = \\begin{cases} +1 & z > 0 \\\\0 & z=0\\\\-1 & z<0 \\end{cases}$\n",
    "- Esto se conoce como el modelo matemático de una neurona de [McCulloch y Pitts](https://link.springer.com/article/10.1007/BF02478259)\n",
    "    - Las coeficientes del filtro son los pesos sinápticos de las dendritas\n",
    "    - La función no lineal corresponde al axón\n",
    "\n",
    "<img src=\"../images/adaptive-neuron.png\" width=\"400\">\n",
    "\n",
    "<img src=\"../images/adaptive-neuron2.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- El algoritmo para entrenar la neurona artificial se conoce como **algoritmo percetrón**\n",
    "- Asumimos un vector de pesos inicial nulo $\\textbf{w}^{(t=0)} = [0, 0, ..., 0]$\n",
    "- La función de costo en el instante $t$ al presentar el ejemplo $(d_n, u_n)$ \n",
    "$$\n",
    "\\mathcal{L}( \\textbf{w}^{(t)} ) = \\text{max} \\Big(0 ~, - d_n \\langle \\textbf{w}^{(t)}, \\text{concat}(1, u_n) \\rangle \\Big)\n",
    "$$\n",
    "- y su derivada es \n",
    "$$\n",
    "\\frac{d \\mathcal{L}(\\textbf{w}^{(t)} )}{d \\textbf{w}}  = \\begin{cases} 0 & d_n \\langle \\textbf{w}, \\text{concat}(1, u_n) \\rangle \\geq 0 \\\\ - d_n \\text{concat}(1, u_n)  & d_n \\langle \\textbf{w}, \\text{concat}(1, u_n) \\rangle < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "es decir que la derivada es cero si el ejemplo está bien clasificado\n",
    "- Finalmente la regla perceptron usando SGD con tasa de aprendizaje $\\mu$ es \n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{w}^{(t+1)} &= \\textbf{w}^{(t)} - \\mu \\frac{d \\mathcal{L}(\\textbf{w}^{(t)} )}{d \\textbf{w}} \\nonumber \\\\\n",
    "& = \\textbf{w}^{(t)} + \\begin{cases} \\mu d_n \\text{concat}(1, u_n) & \\\\ 0 & \\text{en otro caso} \\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "es decir que si el ejemplo está bien clasificado los pesos no se actualizan\n",
    "\n",
    "### Detalles del algoritmo perceptrón\n",
    "- En cada iteración se presenta un ejemplo \n",
    "- Se dice que se completa una época de entrenamiento cuando se han presentado los $N$ ejemplos\n",
    "- Presentar los ejemplos en distinto orden en cada época ayuda a evitar sesgos y acelera la convergencia\n",
    "- Detenemos el entrenamiento cuando todos los ejemplos están bien clasificados o al cumplir un cierto número de épocas sin cambio\n",
    "- El algoritmo perceptrón está garantizado a converger en tiempo finito si el problema es **linealmente separable**\n",
    "- Si el problema no es **linealmente separable** la convergencia se puede forzar disminuyendo gradualmente $\\mu$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "N = 50\n",
    "u = np.concatenate((np.random.randn(N, 2), 2 + np.random.randn(N, 2)))\n",
    "d = np.ones(shape=(2*N,)); d[:N] = -1.\n",
    "w = np.zeros(shape=(3, ))\n",
    "ax.scatter(u[:N, 0], u[:N, 1]); ax.scatter(u[N:, 0], u[N:, 1])\n",
    "x_plot = np.linspace(np.amin(u), np.amax(u))\n",
    "plane = ax.plot(x_plot, -w[0]/(w[2]+1e-10) - x_plot*w[1]/(w[2]+1e-10), 'k-', lw=4, alpha=0.75) \n",
    "P = np.random.permutation(2*N)\n",
    "dot = ax.plot([], [], 'ko', markersize=10)\n",
    "mu = 1e-5\n",
    "\n",
    "def update(n):\n",
    "    global w\n",
    "    idx = P[n - 2*N*int(n / (2*N))]\n",
    "    if d[idx]*np.dot(w, np.concatenate((np.array([1]), u[idx]))) <= 0.:\n",
    "        w = w + mu*d[idx]*np.concatenate((np.array([1]), u[idx]))\n",
    "    dot[0].set_data(u[idx, 0], u[idx, 1])\n",
    "    plane[0].set_ydata(-w[0]/(w[2]+1e-10) - x_plot*w[1]/(w[2]+1e-10))\n",
    "    ax.set_title(\"Iteration %d\" %(n))\n",
    "        \n",
    "\n",
    "anim = animation.FuncAnimation(fig, update, frames=300, interval=100, repeat=False, blit=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Más allá del perceptron \n",
    "\n",
    "- El modelo de neurona con salida sigmoide se conoce como **regresión logística**\n",
    "- Tanto el perceptrón como el regresor logístico se pueden extender a más de dos clases: **regresor softmax**\n",
    "- Conectando varias neuronas en cadena se forma lo que se conoce como una perceptrón multicapa\n",
    "- El perceptrón multicapa es un ejemplo de **red neuronal artificial**\n",
    "- Las redes neuronales artificiales se estudiarán en detalle en el curso de **inteligencia artificial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tópicos extra\n",
    "\n",
    "- [On the Intrinsic Relationship Between the Least Mean Square and Kalman Filters](https://www.commsp.ee.ic.ac.uk/~mandic/LMS_Kalman_IEEE_SPM_2015.pdf)\n",
    "- [Adaptive notch filter](http://homes.esat.kuleuven.be/~tvanwate/courses/dsp2/1415/DSP2_slides_04_adaptievefiltering.pdf)\n",
    "- [Reconstruction using Wiener filter](https://wwwmpa.mpa-garching.mpg.de/~ensslin/lectures/Files/Wiener_Filter_Demo_NIFTy3.html)\n",
    "- [Kalman filter](https://scipy-cookbook.readthedocs.io/items/KalmanFiltering.html)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
