{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import scipy.fft as sfft\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "from IPython.display import YouTubeVideo, HTML, Audio\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import CustomJS, ColumnDataSource, Slider\n",
    "from bokeh.plotting import Figure, show, output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimadores adaptivos parte I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Hasta ahora hemos estudiando sistemas lineales donde: \n",
    "- sus coeficientes quedan fijos luego del diseño y son constantes en el tiempo\n",
    "- hacen supuestos sobre los estadísticos de la señal/ruido\n",
    "\n",
    "Qué hacer si\n",
    "- no podemos hacer supuestos sobre los estadísticos\n",
    "- los estadísticos de la señal/ruido cambian en el tiempo (no estacionaridad)\n",
    "- estamos en un escenario donde los datos llegan continuamente (streaming)\n",
    "\n",
    "Estimador **adaptivo**: \n",
    "\n",
    "- Sistemas cuyos coeficientes se pueden adaptar a medida que llegan nuevos datos\n",
    "- Se diseñan de acuerdo a un método de optimización *online*\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../images/adaptive-systems1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradiente descendente\n",
    "\n",
    "\n",
    "- Sea un vector de pesos $w$ de largo $L+1$ que guarda los coeficientes de un filtro\n",
    "- Sea ahora una función de costo que mapea el vector de pesos a un número real: $J(w): \\mathbb{R}^{L+1} \\to \\mathbb{R}$\n",
    "    - A menor $J$ mejor es nuestro filtro (menor error)\n",
    "\n",
    "Para entrenar un filtro adaptivo \n",
    "1. Partimos de una solución inicial $w_0$\n",
    "1. Modificamos iterativamente $w$ tal que $J(w_{t+1}) < J(w_t)$\n",
    "1. Nos detenemos al cumplir un cierto criterio \n",
    "\n",
    "***\n",
    "Una alternativa de bajo costo para lograr esto es la regla del **gradiente descendente** (GD)\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\mu \\frac{dJ(w)}{dw},\n",
    "$$\n",
    "\n",
    "donde $\\mu$ se conoce como tasa de aprendizaje o \"paso\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(1, figsize=(7, 3))\n",
    "x = np.linspace(-4, 6, num=100)\n",
    "L = lambda x : (x-1)**2 + 3*np.sin(np.pi*x)\n",
    "p = 10*np.random.rand(10) - 4.\n",
    "ax.plot(x, L(x))\n",
    "sc = ax.scatter(p, L(p), s=100)\n",
    "mu = 0.01\n",
    "\n",
    "def update(n):\n",
    "    p = sc.get_offsets()[:, 0]\n",
    "    p = p - mu*2*(p-1) - mu*3*np.pi*np.cos(np.pi*p)\n",
    "    sc.set_offsets(np.c_[p, L(p)])\n",
    "    \n",
    "anim = animation.FuncAnimation(fig, update, frames=100, interval=200, repeat=False, blit=True)\n",
    "#plt.close(); HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***\n",
    "\n",
    "- Imaginemos $J$ como una superficie de $L+1$ dimensiones\n",
    "- En cada punto el gradiente negativo de $J$ nos indica hacia donde está el descenso más abrupto\n",
    "- La tasa $\\mu$ nos da el largo del salto entre $w_t$ y $w_{t+1}$\n",
    "\n",
    "Notemos de la **expansión de Taylor de primer orden** de $J$ en $w_{t}$ que\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J(w_{t+1}) &= J(w_t) + \\frac{dJ(w_t)}{dw} (w_{t+1} - w_{t})  \\nonumber \\\\\n",
    "&= J(w_t) -\\mu \\left \\| \\frac{dJ(w_t)}{dw} \\right \\|^2 \\leq J(w_t) \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "es decir que dado usando la regla GD con $\\mu>0$ se cumple que $J$ decrece monotonicamente\n",
    "\n",
    "- Relación con método de Newton!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradiente descendente en el filtro de Wiener\n",
    "\n",
    "Para el filtro de Wiener teniamos\n",
    "$$\n",
    "J(h) = \\sigma_d^2 - 2 \\textbf{h}^T R_{ud} + \\textbf{h}^T R_{uu} \\textbf{h},\n",
    "$$\n",
    "por ende\n",
    "$$\n",
    "\\frac{dJ(h)}{dh} = -2 R_{ud} + 2 R_{uu} \\textbf{h}\n",
    "$$\n",
    "y finalmente\n",
    "$$\n",
    "\\textbf{h}_{t+1} = \\textbf{h}_{t} (I - 2 \\mu R_{uu}) + 2\\mu R_{ud}\n",
    "$$\n",
    "En este caso la condición de convergencia estable es \n",
    "$$\n",
    "0 < \\mu < \\frac{1}{\\lambda_{\\text{max}}},\n",
    "$$\n",
    "donde $\\lambda_{\\text{max}}$ es valor propio más grande de $R_{uu}$\n",
    "\n",
    "Esto último viene de formar una ecuación de diferencia del estilo $\\hat w_{k, t+1} = (1-\\mu \\lambda_k)^t \\hat w_{k, t=0}$\n",
    "\n",
    "Ref: Haykin, \"Adaptive filter theory\", 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradiente descendente estocástico (SGD)\n",
    "\n",
    "El filtro de Wiener es óptimo pero no adaptivo\n",
    "- Requiere de $N$ muestras de $u$ y $d$ para estimar $R_{ud}$ y $R_{uu}$\n",
    "- Asume estacionaridad: $J(h) = \\mathbb{E}\\left[e_n^2\\right]$\n",
    "- El gradiente descendente (GD) es un método deterministico\n",
    "- Los pesos se adaptan luego de haber presentado las $N$ muestras (batch)\n",
    "\n",
    "Consideremos el caso en que los datos no son estacionarios\n",
    "- Significa que debemos adaptar el filtro en cada paso a medida que nuevas muestras son observadas\n",
    "- Para esto usamos la versión estocástica del GD: SGD\n",
    "- Los pesos se adaptan luego de haber presentado una muestra o un conjunto pequeño (mini-batch)\n",
    "- No hay garantía de llegar al óptimo en un problema convexo, pero es más eficiente computacionalmente que GD\n",
    "\n",
    "<img src=\"../images/adaptive-sgd.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmo Least Mean Square (LMS)\n",
    "\n",
    "\n",
    "Podemos extender el filtro de Wiener al caso no-estacionario usando SGD\n",
    "\n",
    "- El resultado es un algoritmo es simple (filtro FIR) que además es robusto\n",
    "- A diferencia del filtro de Wiener no se requiere conocimiento estadístico del proceso\n",
    "- Tampoco se requiere calcular e invertir la matriz de correlación\n",
    "- Se entrena de manera recursiva y online\n",
    "\n",
    "Consideremos la función de costo estocástica para la arquitectura FIR\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J^s_n(\\textbf{w}) &= e_n^2 \\nonumber \\\\\n",
    "&= (d_n - y_n)^2 \\nonumber \\\\\n",
    "&= (d_n - \\textbf{w}^T \\textbf{u}_n )^2 \\nonumber \\\\\n",
    "&= (d_n - \\sum_{k=0}^{L} w_{n, k} u_{n-k} )^2 \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "donde definimos $\\textbf{u}_n = [u_n, u_{n-1}, \\ldots, u_{n-L}]$ \n",
    "\n",
    "Notemos que usamos el error cuadrático instantaneo en lugar del MSE (filtro de Wiener)\n",
    "\n",
    "El gradiente en función del peso $w_{n, k}$ es \n",
    "$$\n",
    "\\frac{d J^s_n (\\textbf{w})}{d w_{n, k}} = - 2 e_n u_{n-k}\n",
    "$$\n",
    "Usando la regla SGD llegamos a \n",
    "$$\n",
    "w_{n+1, k} = w_{n, k} + 2 \\mu e_n u_{n-k}, k=0, 1, \\ldots, L\n",
    "$$\n",
    "o en forma matricial\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{w}_{n+1} &= \\textbf{w}_{n} + 2 \\mu e_n \\textbf{u}_{n}\\nonumber \\\\\n",
    "&= \\textbf{w}_{n} + 2 \\mu (d_n -  \\textbf{w}_{n}^T \\textbf{u}_{n}) \\textbf{u}_{n}, \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Estimamos la matriz de correlación instantanea y actualizamos los pesos recursivamente\n",
    "- La complejidad de este algoritmo es $L+1$, es decir la complejidad del filtro\n",
    "- Esto se conoce como algoritmo LMS o regla Widrow-Hoff\n",
    "- Inventado en 1960 por [Bernard Widrow](https://en.wikipedia.org/wiki/Bernard_Widrow) y Ted Hoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Interpretación geométrica del algoritmo LMS\n",
    "\n",
    "Tenemos la siguiente regla iterativa\n",
    "$$\n",
    "\\textbf{w}_{n+1} = \\textbf{w}_{n} + 2 \\mu e_n \\textbf{u}_{n} = \\textbf{w}_{n} + \\Delta \\textbf{w}_n\n",
    "$$\n",
    "que se puede interpretar graficamente como\n",
    "\n",
    "<img src=\"../images/adaptive-lms-geometry.png\" width=\"400\">\n",
    "\n",
    "Notemos que\n",
    "\n",
    "- Los cambios en el vector de peso $\\Delta \\textbf{w}_n$ son paralelos a $\\textbf{u}_{n}$\n",
    "- Estos cambios podrían estar dominados por $\\max_k \\textbf{u}_{n} = [u_n, u_{n-1}, \\ldots, u_{n-L}]$\n",
    "- El algoritmo Normalized LMS (NLMS) corrige esto ponderando por la varianza de $\\textbf{u}_{n}$ \n",
    "$$\n",
    "\\textbf{w}_{n+1} = \\textbf{w}_{n} + 2 \\mu e_n \\frac{\\textbf{u}_{n}}{\\left(\\|\\textbf{u}_{n}\\|^2 + \\delta\\right)}\n",
    "$$\n",
    "donde la constante $\\delta$ es un valor pequeño que se usa para evitar divisiones por cero\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LMS: Adaptive line enhancement (ALE)\n",
    "\n",
    "- Sistema adaptivo para eliminar ruido aditivo de un canal\n",
    "- El sistema aprende un filtro pasabanda en torno a la frecuencia de interés\n",
    "- Notece como es posible filtrar incluso ante cambios bruscos en la señal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.lib.stride_tricks import as_strided\n",
    "fig, ax = plt.subplots(3, figsize=(9, 5))\n",
    "t, dt = np.linspace(0, 5, num=500, retstep=True)\n",
    "u = np.sin(2.0*np.pi*t*5)\n",
    "#u[250:] += 5  # Cambio abrupto en la media\n",
    "#u += 2*t  #  Tendencia lineal\n",
    "#u += u*(0.5 + 0.5*np.cos(2.0*np.pi*t/2))  # Tremolo (AM)\n",
    "def update(mu, L, rseed):\n",
    "    np.random.seed(rseed)\n",
    "    u_noise = u + 0.5*np.random.randn(len(t)) \n",
    "    w = np.zeros(shape=(L+1, ))\n",
    "    ax[0].cla(); ax[1].cla(); ax[2].cla(); \n",
    "    #LMS\n",
    "    u_pred = np.zeros(shape=(len(u), ))\n",
    "    for k in range(L+1, len(u)-1):\n",
    "        norm = np.sum(u_noise[k-L-1:k]**2) + 1e-6\n",
    "        u_pred[k] = np.dot(w, u_noise[k-L-1:k])\n",
    "        w += 2*mu*(u_noise[k] - u_pred[k])*u_noise[k-L-1:k]/norm\n",
    "    u_pred[k+1] = np.dot(w, u_noise[k-L-1:k])\n",
    "    ax[0].plot(t, u_noise, 'k.', alpha=0.5); \n",
    "    ax[0].plot(t, u, 'g-', alpha=0.5);  ax[0].plot(t, u_pred); \n",
    "    \n",
    "    ax[1].plot((u - u_pred)**2, label='LMS')\n",
    "    k, Hk = scipy.signal.freqz(b=w, a=1)\n",
    "    ax[2].plot(k/(2*dt*np.pi), np.abs(Hk))\n",
    "    \n",
    "interact(update, mu=SelectionSlider_nice(options=[1e-4, 1e-3, 0.01, 0.1, 0.2, 1.]),\n",
    "         L=SelectionSlider_nice(options=[1, 5, 10, 20, 50], value=10), rseed=IntSlider_nice());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- El algoritmo LMS es un sistema de control con retroalimentación\n",
    "- En el ejemplo anterior notamos la desestabilidad que ocurre con ciertos valores de $\\mu$\n",
    "- La convergencia del algoritmo depende de $\\mu$\n",
    "    - Muy pequeño: Convergencia lenta\n",
    "    - Muy grande: Desestabilidad\n",
    "\n",
    "### Comparación entre Filtro de Wiener/GD y algoritmo LMS/SGD\n",
    "- Wiener: Ambiente estacionario lo cual nos permite calcular $R_{uu}$ y $R_{ud}$. El aprendizaje es determinista.\n",
    "- LMS: El aprendizaje viene promediando a nivel de los estimadores de $w$. Esta sujeto al ruido de los estimadores de gradiente. El aprendizaje es estadístico.\n",
    "- Wiener es óptimo en cambio LMS es sub-óptimo (localmente óptimo). LMS tiende a la solución de Wiener\n",
    "- LMS se actualiza online y tiene costo $L$. Wiener se entrena offline y tiene costo $L^2$\n",
    "\n",
    "<img src=\"../images/adaptive-lms.png\">\n",
    "\n",
    "\n",
    "Convergencia del algoritmo LMS (Haykin 6.5)\n",
    "- El algoritmo LMS tiende en la media $\\mathbb{E}[\\textbf{w}_n] \\to \\textbf{w}^*$ para $n\\to \\infty$\n",
    "- Convergencia en la media cuadrada: La varianza de $\\textbf{w}_n - \\textbf{w}^*$ tiene al valor mínimo de $J$ para $n\\to \\infty$\n",
    "- Esto se cumple si \n",
    "$$\n",
    "0 < \\mu < \\frac{1}{\\lambda_{\\text{max}}}\n",
    "$$\n",
    "donde $\\lambda_{\\text{max}}$ es el valor propio más grande de $R_{uu}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LMS: Cancelación de eco\n",
    "\n",
    "- Supongamos que enviamos una señal de voz a un amig@ \n",
    "- Nuestro amig@ escucha lo que enviamos en un parlante y responde\n",
    "- Nosotros escuchamos la respuesta de nuestro amig@ y adicionalmente nuestro mensaje original\n",
    "\n",
    "<img src=\"../images/adaptive-echo-canceller.png\" width=\"500\">\n",
    "\n",
    "- Podemos usar un filtro adaptivo para cancelar el eco\n",
    "- Usamos como entrada la señal enviada y como salida deseada la señal recibida (con eco)\n",
    "- El filtro aprende el sistema reverberante\n",
    "- El error es la nueva señal recibida limpia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# la señal enviada original\n",
    "r, fs = sf.read(\"data/hola1.ogg\")\n",
    "Audio(r, rate=int(fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# El sistema que introduce ecos, por ejemplo una sala\n",
    "h = np.concatenate(([1.0], np.zeros(10), [0.7], np.zeros(10), [0.5], \n",
    "                    np.zeros(10), [0.3], np.zeros(10), [0.1], np.zeros(10), [0.05]))\n",
    "# la señal enviada con eco\n",
    "rh = np.convolve(r, h)[:len(r)]\n",
    "# la señal recibida limpia \n",
    "s, fs = sf.read(\"data/hola2.ogg\")\n",
    "s = np.concatenate((s, np.zeros(shape=(len(r)-len(s)))))\n",
    "# la señal recibida + señal enviada con eco + ruido blanco\n",
    "srh = s + 0.3*rh + np.random.randn(len(s))*0.005\n",
    "Audio(srh, rate=int(fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "L, mu = 100, 0.02\n",
    "fig, ax = plt.subplots(2, figsize=(9, 4))\n",
    "w = np.zeros(shape=(L+1, ))\n",
    "rhhat = np.zeros(shape=(len(srh), ))\n",
    "for k in range(L+1, len(srh)-1):\n",
    "    norm = np.sum(r[k-L-1:k]**2) + 1e-10\n",
    "    rhhat[k] = np.dot(w, r[k-L-1:k])\n",
    "    w += 2*mu*(srh[k] - rhhat[k])*r[k-L-1:k]/(norm)\n",
    "rhhat[k+1] = np.dot(w, r[k-L-1:k])\n",
    "shat = srh - rhhat\n",
    "ax[0].plot(srh, alpha=0.5, label='hola2+hola1');\n",
    "ax[0].plot(shat, alpha=0.75, label='error');\n",
    "ax[0].plot(s, alpha=0.75, label='hola2 puro');\n",
    "ax[0].legend()\n",
    "ax[1].plot((s - shat)**2)\n",
    "Audio(shat, rate=int(fs))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
