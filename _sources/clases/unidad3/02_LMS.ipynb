{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import scipy.fft as sfft\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "from IPython.display import YouTubeVideo, HTML, Audio\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import CustomJS, ColumnDataSource, Slider\n",
    "from bokeh.plotting import Figure, show, output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimadores adaptivos parte I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Hasta ahora hemos estudiando sistemas lineales donde: \n",
    "- sus coeficientes quedan fijos luego del diseño y son constantes en el tiempo\n",
    "- hacen supuestos sobre los estadísticos de la señal/ruido\n",
    "\n",
    "Qué hacer si\n",
    "- no podemos hacer supuestos sobre los estadísticos\n",
    "- los estadísticos de la señal/ruido cambian en el tiempo: **señales no estacionarias**\n",
    "- estamos en un escenario donde los datos llegan continuamente: data streaming\n",
    "\n",
    "En estos casos necesitamos un estimador de tipo **adaptivo**, es decir sistemas y filtros cuyos coeficientes se pueden **adaptar** a medida que llegan nuevos datos\n",
    "\n",
    "Estos estimadores se diseñan de acuerdo a un método de optimización *online*\n",
    "\n",
    "\n",
    "La siguiente figura muestra algunos ejemplos de aplicaciones de sistemas adaptivos\n",
    "\n",
    "<img src=\"../images/adaptive-systems1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradiente descendente\n",
    "\n",
    "Sea un vector de pesos $w$ de largo $L+1$ que guarda los coeficientes de un filtro\n",
    "\n",
    "Sea ahora una función de costo que mapea el vector de pesos a un número real: $J(w): \\mathbb{R}^{L+1} \\to \\mathbb{R}$. La función de costo debe ser tal que a menor $J$ menor sea el error y por ende mejor sea nuestro filtro\n",
    "\n",
    "Para entrenar un filtro adaptivo \n",
    "\n",
    "1. Partimos de una solución inicial $w_0$\n",
    "1. Modificamos iterativamente $w$ tal que $J(w_{t+1}) < J(w_t)$\n",
    "1. Nos detenemos al cumplir un cierto criterio \n",
    "\n",
    "\n",
    "Una forma de lograr que es relativamente eficiente es la regla del **gradiente descendente** (GD)\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\mu \\frac{dJ(w)}{dw},\n",
    "$$\n",
    "\n",
    "donde $\\mu$ se conoce como tasa de aprendizaje o \"paso\"\n",
    "\n",
    "- Imaginemos $J$ como una superficie de $L+1$ dimensiones\n",
    "- En cada punto el gradiente negativo de $J$ nos indica hacia donde está el descenso más abrupto\n",
    "- La tasa $\\mu$ nos da el largo del salto entre $w_t$ y $w_{t+1}$\n",
    "\n",
    "Observando la **expansión de Taylor de primer orden** de $J$ en $w_{t}$ \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J(w_{t+1}) &= J(w_t) + \\frac{dJ(w_t)}{dw} (w_{t+1} - w_{t})  \\nonumber \\\\\n",
    "&= J(w_t) -\\mu \\left \\| \\frac{dJ(w_t)}{dw} \\right \\|^2 \\leq J(w_t) \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "es decir que usando la regla GD con $\\mu>0$ y asumiendo que $J$ es convexo entonces se cumple que $J$ siempre decrece monotonicamente\n",
    "\n",
    "\n",
    "### Ejemplo gráfico\n",
    "\n",
    "La siguiente animación muestra una superficie no convexa de costo con un parámetro unidimensional \n",
    "\n",
    "Cada punto representa una solución que parte desde una posición inicial distinta\n",
    "\n",
    "Observe como evolucionan las tres soluciones con $\\mu=0.05$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = lambda w : (w-1)**2 + 0.2*np.sin(2*np.pi*w) #  Función de costo\n",
    "gradJ = lambda w : 2*(w-1) + 0.2*2*np.pi*np.cos(2*np.pi*w) # Gradiente\n",
    "w = np.linspace(0, 2, num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "mu = 0.05 # Tasa de aprendizaje\n",
    "wt = np.array([0.05, 0.4, 1.9]) # Solución inicial\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "def init():\n",
    "    global wt\n",
    "    ax.cla()\n",
    "    ax.quiver(wt, J(wt), -mu*gradJ(wt), 0.0, scale=1, \n",
    "              scale_units='x', width=0.005)\n",
    "    ax.plot(w, J(w), lw=2, alpha=.5)\n",
    "    ax.scatter(wt, J(wt), c='k', s=20)\n",
    "    ax.set_xlabel('w')\n",
    "    ax.set_ylabel('J(w)')\n",
    "    ax.set_title(f\"Iteration 0\")\n",
    "    \n",
    "def update(n):\n",
    "    global wt\n",
    "    wt = wt - mu*gradJ(wt) # Regla de gradiente descedente\n",
    "    init()\n",
    "    ax.set_title(f\"Iteration {n+1}\")\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update, init_func=init, frames=15, \n",
    "                               interval=1000, repeat=False, blit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "Dependiendo de donde partimos la solución final es distinta. El gradiente descedente puede quedarse \"atorado\" en un mínimo local o en un punto silla\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora observe como evolucionan las tres soluciones con $\\mu=0.5$, es decir 10 veces más grande que el caso anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "mu = 0.5 # Tasa de aprendizaje\n",
    "wt = np.array([0.05, 0.4, 1.9]) # Solución inicial\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(7, 3), tight_layout=True)\n",
    "anim = animation.FuncAnimation(fig, update, init_func=init, frames=15, \n",
    "                               interval=1000, repeat=False, blit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "Si la tasa de aprendizaje es muy alta, los pasos son muy largos y podríamos no converger a un punto estacionario\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradiente descendente en el filtro de Wiener\n",
    "\n",
    "Para el filtro de Wiener teniamos\n",
    "\n",
    "$$\n",
    "J(h) = \\sigma_d^2 - 2 \\textbf{h}^T R_{ud} + \\textbf{h}^T R_{uu} \\textbf{h},\n",
    "$$\n",
    "\n",
    "por ende\n",
    "\n",
    "$$\n",
    "\\frac{dJ(h)}{dh} = -2 R_{ud} + 2 R_{uu} \\textbf{h}\n",
    "$$\n",
    "\n",
    "y finalmente\n",
    "\n",
    "$$\n",
    "\\textbf{h}_{t+1} = \\textbf{h}_{t} (I - 2 \\mu R_{uu}) + 2\\mu R_{ud}\n",
    "$$\n",
    "\n",
    "En este caso la condición para una convergencia estable es \n",
    "\n",
    "$$\n",
    "0 < \\mu < \\frac{1}{\\lambda_{\\text{max}}},\n",
    "$$\n",
    "\n",
    "donde $\\lambda_{\\text{max}}$ es valor propio más grande de $R_{uu}$ \n",
    "\n",
    "(La prueba de esto puede encontrarse en *Haykin, \"Adaptive filter theory\", Sección 4.3*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradiente descendente estocástico (SGD)\n",
    "\n",
    "El filtro de Wiener es óptimo pero no adaptivo:\n",
    "\n",
    "- Requiere de $N$ muestras de $u$ y $d$ para estimar $R_{ud}$ y $R_{uu}$\n",
    "- Los pesos se adaptan luego de haber presentado las $N$ muestras: Es una estrategia de tipo **batch**\n",
    "- Asume que la señal es estacionaria\n",
    "\n",
    "\n",
    "Consideremos el caso en que los datos no son estacionarios\n",
    "\n",
    "- Significa que debemos adaptar el filtro en cada paso a medida que nuevas muestras son observadas\n",
    "- Para lograr esto podemos usar la versión estocástica del GD: SGD\n",
    "- En SGD los pesos se adaptan luego de haber presentado una sola muestra o un conjunto pequeño de muestras (mini-batch)\n",
    "- En SGD no hay garantía de llegar al óptimo en un problema convexo, pero es más eficiente computacionalmente que GD\n",
    "\n",
    "El siguiente esquema muestra una comparación entre la trayectoria de $w$ cuando se usa GD (negro) y SGD (rojo)\n",
    "\n",
    "En general la trayectoria de SGD será más ruidosa y también podría requerir más pasos, pero cada paso es mucho más económico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/adaptive-sgd.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmo Least Mean Square (LMS)\n",
    "\n",
    "\n",
    "Podemos extender el filtro de Wiener al caso no-estacionario usando SGD, el resultado es un algoritmo simple que además es robusto: El algoritmo LMS\n",
    "\n",
    "- A diferencia del filtro de Wiener no se requiere conocimiento estadístico del proceso\n",
    "- Tampoco se requiere calcular e invertir la matriz de correlación\n",
    "- Se entrena de manera recursiva y online\n",
    "\n",
    "Consideremos la función de costo **estocástica** para la arquitectura FIR\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J^s_n(\\textbf{w}) &= e_n^2 \\nonumber \\\\\n",
    "&= (d_n - y_n)^2 \\nonumber \\\\\n",
    "&= (d_n - \\textbf{w}^T \\textbf{u}_n )^2 \\nonumber \\\\\n",
    "&= (d_n - \\sum_{k=0}^{L} w_{n, k} u_{n-k} )^2 \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde definimos $\\textbf{u}_n = [u_n, u_{n-1}, \\ldots, u_{n-L}]$. \n",
    "\n",
    "Notemos que a diferencia del filtro de Wiener no aplicamos el valor esperado al error cuadrático. Se usa el error cuadrático instantaneo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para continuar calculamos el gradiente en función del peso $w_{n, k}$ \n",
    "\n",
    "$$\n",
    "\\frac{d J^s_n (\\textbf{w})}{d w_{n, k}} = - 2 e_n u_{n-k}\n",
    "$$\n",
    "\n",
    "Luego, usando la regla SGD llegamos a \n",
    "\n",
    "$$\n",
    "w_{n+1, k} = w_{n, k} + 2 \\mu e_n u_{n-k}, k=0, 1, \\ldots, L\n",
    "$$\n",
    "\n",
    "y que en forma matricial es\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{w}_{n+1} &= \\textbf{w}_{n} + 2 \\mu e_n \\textbf{u}_{n}\\nonumber \\\\\n",
    "&= \\textbf{w}_{n} + 2 \\mu (d_n -  \\textbf{w}_{n}^T \\textbf{u}_{n}) \\textbf{u}_{n}, \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "que se conoce como la regla de Widrow-Hoff\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "En resumen: Estimamos el error instantaneo y actualizamos los pesos recursivamente\n",
    "    \n",
    "</div>\n",
    "\n",
    "La complejidad de este algoritmo es $L+1$, es decir la complejidad del filtro\n",
    "\n",
    "Este algoritmo fue inventado en 1960 por [Bernard Widrow](https://en.wikipedia.org/wiki/Bernard_Widrow) y Ted Hoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergencia del algoritmo LMS (Haykin 6.5)\n",
    "\n",
    "El algoritmo LMS tiende en la media al valor óptimo\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\textbf{w}_n] \\to \\textbf{w}^*\n",
    "$$ \n",
    "\n",
    "para $n\\to \\infty$\n",
    "\n",
    "Además convergence en la media cuadrada: La varianza de $\\textbf{w}_n - \\textbf{w}^*$ tiene al valor mínimo de $J$ para $n\\to \\infty$\n",
    "\n",
    "Esto se cumple si \n",
    "\n",
    "$$\n",
    "0 < \\mu < \\frac{2}{\\text{Tr}[R_{uu}]}\n",
    "$$\n",
    "\n",
    "donde $R_{uu} = \\mathbb{E}[\\textbf{u}_n \\textbf{u}_n^T ]$ es la matriz de autocorrelación y $\\text{Tr}[]$ el operador que calcula la traza de una matriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Algoritmo Normalized LMS (NLMS)\n",
    "\n",
    "Tenemos la siguiente regla iterativa\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{w}_{n+1} &= \\textbf{w}_{n} + 2 \\mu (d_n -  \\textbf{w}_{n}^T \\textbf{u}_{n}) \\textbf{u}_{n} \\nonumber \\\\\n",
    "& = \\textbf{w}_{n} + \\Delta \\textbf{w}_n \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "que se puede interpretar graficamente como\n",
    "\n",
    "<img src=\"../images/adaptive-lms-geometry.png\" width=\"400\">\n",
    "\n",
    "(donde $\\textbf{x}(k)$ y $\\textbf{w}(k)$ corresponden a $\\textbf{u}_n$ y $\\textbf{w}_n$ en nuestra notación, respectivamente)\n",
    "\n",
    "Notemos que\n",
    "\n",
    "- Los cambios en el vector de peso $\\Delta \\textbf{w}_n$ son paralelos a $\\textbf{u}_{n}$\n",
    "- Estos cambios podrían estar dominados por \n",
    "\n",
    "$$\n",
    "\\max \\textbf{u}_{n} = [u_n, u_{n-1}, \\ldots, u_{n-L}]\n",
    "$$\n",
    "\n",
    "- El algoritmo Normalized LMS (NLMS) corrige este problema ponderando por la varianza de $\\textbf{u}_{n}$ \n",
    "\n",
    "$$\n",
    "\\textbf{w}_{n+1} = \\textbf{w}_{n} + 2 \\mu (d_n -  \\textbf{w}_{n}^T \\textbf{u}_{n}) \\frac{\\textbf{u}_{n}}{\\left(\\|\\textbf{u}_{n}\\|^2 + \\delta\\right)}\n",
    "$$\n",
    "\n",
    "donde la constante $\\delta$ es un valor pequeño que se usa para evitar divisiones por cero\n",
    "\n",
    "\n",
    "En lo que sigue usaremos NLMS para revisar algunas aplicaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementación del filtro NLMS y ejemplos de aplicaciones\n",
    "\n",
    "\n",
    "Podemos implementar las ecuaciones del filtro NLMS como se muestra a continuación\n",
    "\n",
    "El filtro recibe como entrada el orden $L$ y la tasa de aprendizaje $\\mu$\n",
    "\n",
    "Se asume un vector cero para los pesos iniciales, pero también en la práctica podemos partir de una solución anterior si esta existiera \n",
    "\n",
    "Para actualizar el vector de pesos es necesario entregar el vector $\\textbf{u}_n \\in \\mathbb{R}^{L+1}$ y la salida deseada $d_n \\in \\mathbb{R}$. Luego de actualizar se retorna el valor de la salida predicha por el filtro \n",
    "\n",
    "$$\n",
    "y_n = w_n^T \\textbf{u}_n\n",
    "$$\n",
    "\n",
    "Estudie el código y compare con las ecuaciones presentadas anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Filtro_NLMS:\n",
    "    \n",
    "    def __init__(self, L, mu, delta=1e-6, winit=None):\n",
    "        self.L = L\n",
    "        self.w = np.zeros(shape=(L+1, ))\n",
    "        self.mu = mu\n",
    "        self.delta = delta\n",
    "        \n",
    "    def update(self, un, dn):\n",
    "        # Asumiendo que un = [u[n], u[n-1], ..., u[n-L]]\n",
    "        unorm = np.dot(un, un) + self.delta\n",
    "        yn = np.dot(self.w, un)\n",
    "        self.w += 2*self.mu*(dn - yn)*(un/unorm)\n",
    "        return yn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación probaremos este filtro con una aplicación conocida como **Adaptive line enhancement** (ALE).\n",
    "\n",
    "ALE se refiere a un sistema adaptivo para eliminar ruido blanco aditivo de una señal. El sistema aprende un filtro pasabanda en torno a la frecuencia de interés\n",
    "\n",
    "En ALE usamos como señal deseada \n",
    "\n",
    "$$\n",
    "d_n = u_n = \\textbf{u}_n[0]\n",
    "$$\n",
    "\n",
    "El valor predicho por el filtro será la señal $u$ pero libre de ruido blanco. Esto se debe a que el ruido blanco no tiene correlación y por ende no el filtro adaptivo no lo puede predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digamos que u = s + n\n",
    "# El objetivo es limpiar u para obtener s\n",
    "# s es una señal determínista y n es ruido blanco\n",
    "\n",
    "Fs, f0 =  50, 5\n",
    "t = np.arange(0, 4, 1/Fs)\n",
    "s = np.sin(2.0*np.pi*t*f0)\n",
    "n = 0.5*np.random.randn(len(t)) \n",
    "s[t>2.0] += 5  # Simulemos un cambio abrupto en la media de la señal\n",
    "#s += s*(0.5 + 0.5*np.cos(2.0*np.pi*t/2))  # Tremolo (AM)\n",
    "u = s + n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diferencia de un filtro estático (como el filtro de Wiener) es posible filtrar incluso ante cambios bruscos en la señal. \n",
    "\n",
    "Veamos como cambia el resultado del filtro con distintos valores de $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import Dark2_5 as palette\n",
    "\n",
    "L = 20\n",
    "mus =  np.logspace(-2, 0, num=5)\n",
    "\n",
    "u_pred = np.zeros(shape=(len(u), len(mus)))\n",
    "for i, mu in enumerate(mus):\n",
    "    myfilter = Filtro_NLMS(L=L, mu=mu)\n",
    "    for k in range(L+1, len(u)):\n",
    "        u_pred[k, i] = myfilter.update(u[k-L-1:k][::-1], u[k])\n",
    "\n",
    "p = [Figure(plot_width=620, plot_height=230, toolbar_location=\"below\") for mu in mus]\n",
    "for i, mu in enumerate(mus):\n",
    "    p[i].line(t, s, color='green', line_width=2, legend_label=f\"Señal limpia\");  \n",
    "    p[i].scatter(t, u, color='black', legend_label=f\"Señal ruidosa\")\n",
    "    p[i].line(t, u_pred[:, i], color='blue', \n",
    "           line_width=2, legend_label=f\"Señal filtrada\"); \n",
    "    p[i].title.text = f\"Tasa de aprendizaje: {mu:0.4f}\"\n",
    "    p[i].legend.location = \"top_left\"\n",
    "    \n",
    "show(column(p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "La tasa de aprendizaje $\\mu$ controla la velocidad de adaptación. Pero una tasa demasiado grande provoca que el filtro sea inestable. \n",
    "    \n",
    "En general el valor óptimo de $\\mu$ depende del problema y del valor de $L$\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 20\n",
    "myfilter = Filtro_NLMS(L=20, mu=0.02)\n",
    "w = np.zeros()\n",
    "for k in range(L+1, len(u)):\n",
    "    u_pred[k, i] = myfilter.update(u[k-L-1:k][::-1], u[k])\n",
    "        \n",
    "fk, Hk = scipy.signal.freqz(b=myfilter.w, a=1, fs=Fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen de comparación entre Filtro de Wiener/GD y algoritmo LMS/SGD\n",
    "\n",
    "- **Supuestos**:  Wiener requiere un ambiente estacionario lo cual nos permite calcular $R_{uu}$ y $R_{ud}$. En LMS la señal puede ser no estacionaria.\n",
    "- **Aprendizaje:** En el filtro de Wiener el aprendizaje es determinista. En LMS el aprendizaje viene **promediando** a nivel de los estimadores de $w$. En LMS el aprendizaje es estadístico. \n",
    "- **Optimalidad:** Wiener es óptimo en cambio LMS es sub-óptimo (localmente óptimo). LMS tiende a la solución de Wiener\n",
    "- **Costo:** LMS se actualiza online y tiene costo $L$. Wiener se entrena offline y tiene costo $L^2$\n",
    "\n",
    "Diagrama que compara el filtro de Wiener y el algoritmo LMS\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../images/adaptive-lms.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INFO183",
   "language": "python",
   "name": "info183"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
