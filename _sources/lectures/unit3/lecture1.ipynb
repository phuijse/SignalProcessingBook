{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "hv.opts.defaults(hv.opts.Curve(width=500), \n",
    "                 hv.opts.Points(width=500), \n",
    "                 hv.opts.Image(width=500, colorbar=True, cmap='Viridis'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimador lineal óptimo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Un **estimador** es un sistema diseñado para **extraer información** a partir de una **señal**\n",
    "\n",
    "- La señal contiene **información y ruido** \n",
    "- La señal es representada como una secuencia de **datos**\n",
    "\n",
    "Tipos de estimador\n",
    "\n",
    "- **Filtro:** Estimo el valor actual de mi señal acentuando o eliminando una o más características\n",
    "- **Predictor:** Estimo el valor futuro de mi señal\n",
    "\n",
    "En esta lección estudiaremos estimadores lineales y óptimos\n",
    "\n",
    "- Lineal: La cantidad estimada es una función lineal de la entrada\n",
    "- Óptimo: El estimador es la mejor solución posible de acuerdo a un criterio\n",
    "\n",
    "Para entender los fundamentos de los estimadores óptimos es necesario introducir el concepto de proceso aleatorio. Luego estudiaremos uno de los estimadores óptimos más importantes: El filtro de Wiener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Proceso aleatorio o proceso estocástico\n",
    "\n",
    "Un proceso estocástico es una **colección de variables aleatorias** indexadas tal que forman una secuencia. Se denotan matemáticamente como un conjunto $\\{U_k\\}$, con $k=0, 1, 2, \\ldots, N$. El índice $k$ puede representar tiempo, espacio u otra variable independiente.\n",
    "\n",
    "La siguiente figura muestra tres realizaciones u observaciones de un proceso estocástico con cuatro elementos\n",
    "\n",
    "<img src=\"../images/stochastic_process.png\" width=\"600\">\n",
    "\n",
    "Existen muchos fenómenos cuya evolución se modela utilizando procesos aleatorios. Por ejemplo\n",
    "\n",
    "- Los índices bursatiles\n",
    "- El comportamineto de un gas dentro de un contenedor\n",
    "- Las vibraciones de un motor eléctrico\n",
    "- El área de una célula durante un proceso de organogénesis\n",
    "\n",
    "A continuación revisaremos algunas de las propiedades de los procesos aleatorios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Momentos de un proceso estocástico**\n",
    "\n",
    "Un proceso aleatorio $U_n = (u_n, u_{n-1}, u_{n-2}, \\ldots, u_{n-L})$ se describe a través de sus momentos estadísticos. Si consideramos una caracterízación de segundo orden necesitamos definir\n",
    "\n",
    "- Momento central o media: Describe el valor central del proceso\n",
    "\n",
    "$$\n",
    "\\mu(n) = \\mathbb{E}[U_n]\n",
    "$$\n",
    "\n",
    "- Segundo momento o correlación: Describe la dispersión de un proceso \n",
    "\n",
    "$$\n",
    "r_{uu}(n, n-k) = \\mathbb{E}[U_n U_{n-k}]\n",
    "$$\n",
    "\n",
    "- Segundo momento centrado o covarianza\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "c_{uu}(n, n-k) &= \\mathbb{E}[(U_n-\\mu_n) (U_{n-k}- \\mu_{n-k})] \\nonumber \\\\\n",
    "&= r(n,n-k) - \\mu_n \\mu_{n-k} \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Correlación cruzada entre dos procesos \n",
    "\n",
    "$$\n",
    "r_{ud}(n, n-k) = \\mathbb{E}[U_n D_{n-k}]\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proceso estacionario y ergódico**\n",
    "\n",
    "En esta lección nos vamos a centrar en el caso simplificado donde el **proceso es estacionario**, matemáticamente esta propiedad significa que\n",
    "\n",
    "$$\n",
    "\\mu(n)  = \\mu, \\forall n\n",
    "$$\n",
    "\n",
    "y\n",
    "\n",
    "$$\n",
    "r_{uu}(n, n-k)  = r_{uu}(k), \\forall n\n",
    "$$\n",
    "\n",
    "es decir que los momentos estadísticos se mantienen constantes en el tiempo (no depende de $n$).\n",
    "\n",
    "Otra simplificación que utilizaremos es que el proceso sea **ergódico**, \n",
    "\n",
    "$$\n",
    "\\mathbb{E}[U_n] = \\frac{1}{N} \\sum_{n=1}^N u_n\n",
    "$$\n",
    "\n",
    "es decir que podemos reemplazar el valor esperado por la media muestral en el tiempo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Densidad espectral de potencia**\n",
    "\n",
    "La densidad espectral de potencia o *power spectral density* (PSD) mide la distribución en frecuencia de la potencia del proceso estocástico. Su definición matemática es\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "S_{uu}(f) &= \\sum_{k=-\\infty}^{\\infty} r_{uu}(k) e^{-j 2\\pi f k} \\nonumber \\\\\n",
    "&= \\lim_{N\\to\\infty} \\frac{1}{2N+1} \\mathbb{E} \\left [\\left|\\sum_{n=-N}^{N} u_n e^{-j 2\\pi f n} \\right|^2 \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "que corresponde a la transformada de Fourier de la correlación (caso estacionario)\n",
    "\n",
    "La PSD y la correlación forman un par de Fourier, es decir que uno es la transformada de Fourier del otro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Filtro de Wiener\n",
    "\n",
    "El filtro de Wiener fue publicado por Norbert Wiener en 1949 y es tal vez el ejemplo más famoso de un estimador lineal óptimo.\n",
    "\n",
    ":::{important}\n",
    "\n",
    "Para diseñar un estimador óptimo necesitamos un **criterio** y **condiciones** (supuestos). Luego el estimador será **óptimo según dicho criterio y bajo los supuestos considerados**. Por ejemplo podríamos suponer un escenario donde el ruido es blanco o donde el proceso es estacionario.\n",
    "\n",
    ":::\n",
    "\n",
    "A continuación describiremos en detalle este filtro y explicaremos como se optimiza. Luego se veran ejemplos de aplicaciones.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notación y arquitectura del filtro de Wiener\n",
    "\n",
    "El filtro de Wiener es un sistema de tiempo discreto con estructura FIR y $L+1$ coeficientes. A continuación se muestra un esquema del filtro de Wiener\n",
    "\n",
    "<img src=\"../images/wiener.png\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del esquema podemos reconocer los elementos más importantes de este filtro\n",
    "\n",
    "- Los coeficientes del filtro: $h_0, h_1, h_2, \\ldots, h_{L}$\n",
    "- La señal de entrada al filtro: $u_0, u_1, u_2, \\ldots$\n",
    "- La señal de salida del filtro: $y_0, y_1, y_2, \\ldots$\n",
    "- La señal de respuesta \"deseada\" u objetivo: $d_0, d_1, d_2, \\ldots$\n",
    "- La señal de error: $e_0, e_1, e_2, \\ldots$\n",
    "\n",
    "Al ser un filtro FIR la salida del filtro está definida como\n",
    "\n",
    "$$\n",
    "y_n = \\sum_{k=0}^{L} h_k u_{n-k},\n",
    "$$\n",
    "\n",
    "es decir la convolución entre la entrada y los coeficientes. Luego la señal de error es\n",
    "\n",
    "$$\n",
    "e_n = d_n - y_n = d_n - \\sum_{k=0}^{L} h_k u_{n-k} \n",
    "$$\n",
    "\n",
    "que corresponde a la diferencia entre la señal objetivo y la señal de salida. \n",
    "\n",
    "A continuación veremos que se ajustan los coeficientes del filtro en base al criterio de optimalidad. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste del filtro de Wiener\n",
    "\n",
    "El criterio más común para aprender o adaptar el filtro de Wiener es el **error medio cuadrático** o *mean square error* (MSE) entre la respuesta deseada y la salida del filtro.\n",
    "\n",
    "Asumiendo que $u$ y $d$ son secuencias de valores reales podemos escribir el MSE como\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{MSE} &= \\mathbb{E}\\left [e_n^2 \\right] \\nonumber \\\\\n",
    "&= \\mathbb{E}\\left [(d_n - y_n)^2 \\right] \\nonumber \\\\\n",
    "&= \\mathbb{E}\\left [d_n^2 \\right]  - 2\\mathbb{E}\\left [ d_n y_n \\right] + \\mathbb{E}\\left [ y_n^2 \\right] \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde $\\sigma_d^2 = \\mathbb{E}\\left [d_n^2 \\right]$ es la varianza de la señal deseada y $\\sigma_y^2 = \\mathbb{E}\\left [ y_n^2 \\right]$ es la varianza de nuestro estimador\n",
    " \n",
    ":::{note}\n",
    "    \n",
    "Minimizar el MSE implica acercar la salida del filtro a la respuesta deseada\n",
    "    \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, igualando la derivada del MSE a cero, tenemos \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{d h_j} \\text{MSE} &= -2\\mathbb{E}\\left[ d_n \\frac{d y_n}{d h_j}  \\right]  + 2 \\mathbb{E}\\left[ y_n \\frac{d y_n}{d h_j}    \\right]  \\nonumber \\\\\n",
    "&= -2\\mathbb{E}\\left[ d_n u_{n-j} \\right]  + 2 \\mathbb{E}\\left[ y_n u_{n-j}    \\right]  \\nonumber \\\\\n",
    "&= -2\\mathbb{E}\\left[ d_n u_{n-j} \\right]  + 2 \\mathbb{E}\\left[ \\sum_{k=0}^{L} h_k u_{n-k}  u_{n-j} \\right] \\nonumber \\\\\n",
    "&= -2\\mathbb{E}\\left[ d_n u_{n-j} \\right]  + 2 \\sum_{k=0}^{L} h_k \\mathbb{E}\\left[ u_{n-k}  u_{n-j} \\right] = 0 \\nonumber \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Si despejamos y repetimos para $j=0, \\ldots, L$ obtenemos el siguiente sistema de ecuaciones\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\n",
    "r_{uu}(0) & r_{uu}(1) & r_{uu}(2) & \\ldots & r_{uu}(L) \\\\\n",
    "r_{uu}(1) & r_{uu}(0) & r_{uu}(1) & \\ldots & r_{uu}(L-1) \\\\\n",
    "r_{uu}(2) & r_{uu}(1) & r_{uu}(0) & \\ldots & r_{uu}(L-2) \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots &\\vdots \\\\\n",
    "r_{uu}(L) & r_{uu}(L-1) & r_{uu}(L-2) & \\ldots & r_{uu}(0) \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "h_0  \\\\\n",
    "h_1  \\\\\n",
    "h_2  \\\\\n",
    "\\vdots  \\\\\n",
    "h_L \\\\\n",
    "\\end{pmatrix} &= \n",
    "\\begin{pmatrix}\n",
    "r_{ud}(0)  \\\\\n",
    "r_{ud}(1)  \\\\\n",
    "r_{ud}(2) \\\\\n",
    "\\vdots  \\\\\n",
    "r_{ud}(L) \\\\\n",
    "\\end{pmatrix} \\nonumber \\\\\n",
    "R_{uu} \\textbf{h} &= R_{ud},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "que se conoce como las **ecuaciones de Wiener-Hopf**. Además $R_{uu}$ se conoce como matriz de auto-correlación. \n",
    "\n",
    "Asumiendo que $R_{uu}$ es no-singular, es decir que su inversa existe, la **solución óptima en el sentido de mínimo MSE** es \n",
    "\n",
    "$$\n",
    "\\textbf{h}^{*} = R_{uu} ^{-1} R_{ud}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que por construcción la matriz $R_{uu}$ es simétrica y hermítica. Por lo que el sistema puede resolverse de forma eficiente con  $\\mathcal{O}(L^2)$ operaciones usando la [recursión de Levison-Durbin](https://en.wikipedia.org/wiki/Levinson_recursion) \n",
    "\n",
    "\n",
    ":::{warning}\n",
    "\n",
    "Para llegar a la solución impusimos dos condiciones sobre la salida deseada y la entrada: (1) tienen media cero y (2) son estacionarias en el sentido amplio (es decir la correlación solo depende del retardo $m$).\n",
    "\n",
    ":::\n",
    "\n",
    "- Si la primera condición no se cumpliera, podría restarse la media previo al entrenamiento del filtro\n",
    "- Si la segunda condición no se cumple conviene usar otro método como los que veremos en las lecciones siguientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicaciones del filtro de Wiener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regresión  o identificación de sistema\n",
    "\n",
    "En regresión buscamos encontrar los coeficientes $h$ a partir de tuplas $(X, Y)$ tal que\n",
    "\n",
    "$$\n",
    "Y = h^T X + \\epsilon,\n",
    "$$\n",
    "\n",
    "donde $X \\in \\mathbb{R}^{N\\times D}$ son las variables dependientes (entrada), $Y \\in \\mathbb{R}^N$ es la  variable dependiente (salida) y $\\epsilon$ es ruido\n",
    "\n",
    "Para entrenar el filtro\n",
    "\n",
    "1. Asumimos que hemos observado N muestras de $X$ e $Y$ \n",
    "1. A partir de $u=X$ construimos $R_{uu}$\n",
    "1. A partir de $d=Y$ construimos $R_{ud}$\n",
    "1. Finalmente recuperamos $\\textbf{h}$ usando $R_{uu} ^{-1} R_{ud}$\n",
    "1. Con esto podemos interpolar $Y$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo** Sea por ejemplo una regresión de tipo polinomial donde queremos encontrar $h_k$ tal que\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "d_i &= f_i + \\epsilon \\nonumber \\\\\n",
    "&= \\sum_{k=1}^L h_k u_i^k + \\epsilon \\nonumber\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "u = np.linspace(-2, 2, num=30)\n",
    "f = 0.25*u**5 - 2*u**3 + 5*u # Los coeficientes reales son [0, 5, 0, -2, 0, 1/4, 0, 0, 0, ...]\n",
    "d = f + np.random.randn(len(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Points((u, d), kdims=['u', 'd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementemos el filtro como una clase con dos métodos públicos `fit` (ajustar) y `predict` (predecir). El filtro tiene un argumento, el número de coeficientes $L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wiener_polynomial_regression:\n",
    "    \n",
    "    def __init__(self, L: int):\n",
    "        self.L = L\n",
    "        self.h = np.zeros(shape=(L+1,))\n",
    "        \n",
    "    def _polynomial_basis(self, u: np.ndarray) -> np.ndarray:\n",
    "        U = np.ones(shape=(len(u), self.L))\n",
    "        for i in range(1, self.L):\n",
    "            U[:, i] = u**i\n",
    "        return U\n",
    "        \n",
    "    def fit(self, u: np.ndarray, d: np.ndarray):\n",
    "        U = self._polynomial_basis(u)\n",
    "        Ruu = np.dot(U.T, U)\n",
    "        Rud = np.dot(U.T, d[:, np.newaxis])\n",
    "        self.h = scipy.linalg.solve(Ruu, Rud, assume_a='pos')[:, 0]\n",
    "        \n",
    "    def predict(self, u: np.ndarray):\n",
    "        U = self._polynomial_basis(u)\n",
    "        return np.dot(U, self.h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "La función `scipy.linalg.solve(A, B)` retorna la solución del sistema de ecuaciones lineal `Ax = B`. El argumento `assume_a` puede usarse para indicar que `A` es simétrica, hermítica o definido positiva.\n",
    "\n",
    ":::\n",
    "\n",
    "Los solución de un sistema con `10` coeficientes es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Wiener_polynomial_regression(10)\n",
    "regressor.fit(u, d)\n",
    "print(regressor.h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo cambia el resultado con L?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uhat = np.linspace(np.amin(u), np.amax(u), num=100)\n",
    "yhat = {}\n",
    "for L in [2, 5, 15]:\n",
    "    regressor = Wiener_polynomial_regression(L)\n",
    "    regressor.fit(u, d)\n",
    "    yhat[L] = regressor.predict(uhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "p = [hv.Points((u, d), kdims=['u', 'd'], label='data').opts(size=4, color='k')]\n",
    "for L, prediction in yhat.items():\n",
    "    p.append(hv.Curve((uhat, prediction), label=f'L={L}'))\n",
    "hv.Overlay(p).opts(legend_position='top') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "    \n",
    "Si $L$ es muy pequeño el filtro es demasiado simple. Si $L$ es muy grande el filtro se puede sobreajustar al ruido\n",
    "    \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Predicción a futuro\n",
    "\n",
    "En este caso asumimos que la señal deseada es la entrada en el futuro\n",
    "\n",
    "$$\n",
    "d_n = \\{u_{n+1}, u_{n+2}, \\ldots, u_{n+m}\\}\n",
    "$$ \n",
    "\n",
    "donde $m$ es el horizonte de predicción. Se llama *predicción a un paso* al caso particular $m=1$.\n",
    "\n",
    "El largo del filtro $L$ define la cantidad de muestras pasadas que usamos para predecir. Por ejemplo un sistema de predicción a un paso con $L+1 = 3$ coeficientes:\n",
    "\n",
    "$$\n",
    "h_0 u_n +  h_1 u_{n-1} + h_2 u_{n-2}= y_n = \\hat u_{n+1} \\approx u_{n+1}\n",
    "$$\n",
    "\n",
    "Para entrenar el filtro\n",
    "\n",
    "1. Asumimos que la señal ha sido observada y que se cuenta con $N$ muestras para entrenar\n",
    "1. Podemos formar una matriz cuyas filas son $[u_n, u_{n-1}, \\ldots, u_{n-L}]$ para $n=L,L+1,\\ldots, N-1$\n",
    "1. Podemos formar un vector $[u_N, u_{N-1}, \\ldots, u_{L+1}]^T$ (caso $m=1$)\n",
    "1. Con esto podemos formar las matrices de correlación y obtener $\\textbf{h}$\n",
    "1. Finalmente usamos $\\textbf{h}$ para predecir el futuro no observado de $u$\n",
    "\n",
    "Nuevamente implementamos el filtro como una clase. Esta vez se utiliza la función de numpy `as_strided` para formar los vectores de \"instantes pasados\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "class Wiener_predictor:\n",
    "    \n",
    "    def __init__(self, L: int):\n",
    "        self.L = L\n",
    "        self.h = np.zeros(shape=(L+1,))\n",
    "      \n",
    "    def fit(self, u: np.ndarray):\n",
    "        U = as_strided(u, [len(u)-self.L+1 , self.L+1], \n",
    "                       strides=[u.strides[0], u.strides[0]])\n",
    "        Ruu = np.dot(U[:, :self.L].T, U[:, :self.L])\n",
    "        Rud = np.dot(U[:, :self.L].T, U[:, self.L][:, np.newaxis])\n",
    "        self.h = scipy.linalg.solve(Ruu, Rud, assume_a='pos')[:, 0]\n",
    "        \n",
    "    def predict(self, u: np.ndarray, m: int=1):\n",
    "        u_pred = np.zeros(shape=(m+self.L, ))\n",
    "        u_pred[:self.L] = u\n",
    "        for k in range(self.L, m+L):\n",
    "            u_pred[k] = np.sum(self.h*u_pred[k-self.L:k])\n",
    "        return u_pred[self.L:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la siguiente señal sinusoidal ¿Cómo afecta $L$ a la calidad del predictor lineal?\n",
    "\n",
    "Utilizaremos los primeros 100 instantes para ajustar y los siguientes 100 para probar el predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "t = np.linspace(0, 10, num=200)\n",
    "u = np.sin(2.0*np.pi*0.5*t) + 0.25*np.random.randn(len(t))\n",
    "N_fit = 100\n",
    "\n",
    "yhat = {}\n",
    "for L in [10, 20, 30]:\n",
    "    predictor = Wiener_predictor(L)\n",
    "    predictor.fit(u[:N_fit])\n",
    "    yhat[L] = predictor.predict(u[N_fit-L:N_fit], m=100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "p = [hv.Points((t, u), ['instante', 'u'], label='Datos').opts(color='k')]\n",
    "for L, prediction in yhat.items():\n",
    "    p.append(hv.Curve((t[N_fit:], prediction), label=f'L={L}'))\n",
    "hv.Overlay(p).opts(legend_position='top') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}    \n",
    "\n",
    "Si $L$ es muy pequeño el filtro es demasiado simple. Si $L$ es muy grande el filtro se puede sobreajustar al ruido   \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Eliminar ruido blanco aditivo\n",
    "\n",
    "En este caso asumimos que la señal de entrada corresponde a una señal deseada (información) que ha sido contaminada con ruido aditivo\n",
    "\n",
    "$$\n",
    "u_n = d_n + \\nu_n,\n",
    "$$\n",
    "\n",
    "adicionalmente asumimos que\n",
    "- el ruido es estacionario en el sentido amplio y de media cero $\\mathbb{E}[\\nu_n] = 0$\n",
    "- el ruido es blanco, es decir no tiene correlación consigo mismo o con la señal deseada\n",
    "\n",
    "$$\n",
    "r_{\\nu d}(k) = 0, \\forall k\n",
    "$$\n",
    "\n",
    "- el ruido tiene una cierta varianza $\\mathbb{E}[\\nu_n^2] = \\sigma_\\nu^2, \\forall n$\n",
    "\n",
    "Notemos que en este caso $R_{uu} = R_{dd} + R_{\\nu\\nu}$ y $R_{ud} = R_{dd}$, luego\n",
    "\n",
    "la señal recuperada es $\\hat d_n = h^{*} u_n$ y el filtro es\n",
    "\n",
    "$$\n",
    "\\vec h^{*} = \\frac{R_{dd}}{R_{dd} + R_{\\nu\\nu}}\n",
    "$$\n",
    "\n",
    "y su respuesta en frecuencia\n",
    "\n",
    "$$\n",
    "H(f) = \\frac{S_{dd}(f)}{S_{dd}(f) + S_{\\nu\\nu}(f)}\n",
    "$$\n",
    "\n",
    "es decir que \n",
    "- en frecuencias donde la $S_{dd}(f) > S_{\\nu\\nu}(f)$, entonces $H(f) = 1$\n",
    "- en frecuencias donde la $S_{dd}(f) < S_{\\nu\\nu}(f)$, entonces $H(f) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
